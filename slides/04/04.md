title: NPFL129, Lecture 4
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Multiclass Logistic Regression, Support Vector Machine

## Milan Straka

### November 11, 2019

---
section: LogisticRegression
# Logistic Regression

An extension of perceptron, which models the conditional probabilities
of $p(C_0|→x)$ and of $p(C_1|→x)$. Logistic regression can in fact
handle also more than two classes, which we will see shortly.

~~~
Logistic regression employs the following parametrization of the conditional
class probabilities:
$$\begin{aligned}
  P(C_1 | →x) &= σ(→x^t →w + →b) \\
  P(C_0 | →x) &= 1 - P(C_1 | →x),
\end{aligned}$$
where $σ$ is a _sigmoid function_
$$σ(x) = \frac{1}{1+e^{-x}}.$$

~~~
Can be trained using an SGD algorithm.

---
# Sigmoid Function

The sigmoid function has values in range $(0, 1)$, is monotonically
increasing and it has a derivative of $\frac{1}{4}$ at $x=0$.

$$σ(x) = \frac{1}{1+e^{-x}}$$

~~~
$$σ'(x) = σ(x) \big(1 - σ(x)\big)$$

~~~
![w=100%](../03/sigmoid.pdf)

---
# Logistic Regression

To give some meaning to the sigmoid function, starting with
$$P(C_1 | →x) = σ(f(→x; →w)) = \frac{1}{1 + e^{-f(→x; →w)}}$$
~~~
we can arrive at
$$f(→x; →w) = \log\left(\frac{P(C_1 | →x)}{P(C_0 | →x)}\right),$$
where the prediction of the model $f(→x; →w)$ is called a _logit_
and it is a logarithm of odds of the two classes probabilities.

---
# Logistic Regression

To train the logistic regression $y(→x; →w) = →x^T →w$, we use MLE (the maximum likelihood
estimation). Note that $P(C_1 | →x; →w) = σ(y(→x; →w))$.

~~~
Therefore, the loss for a batch $𝕏=\{(→x_1, t_1), (→x_2, t_2), …, (→x_N, t_N)\}$
is
$$\begin{aligned}
𝓛(𝕏) = \frac{1}{N} ∑_i -\log(P(C_{t_i} | →x; →w)). \\
\end{aligned}$$

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, +1\}$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(P(C_{t_i} | →x; →w)$
  - $→w ← →w - α→g$
</div>

---
section: MulticlassLogisticRegression
# Multiclass Logistic Regression

To extend the binary logistic regression to a multiclass case with $K$ classes, we:
- Generate multiple outputs, notably $K$ outputs, each with its own set of
  weights, so that
  $$y(→x; ⇉W)_i = →W_i →x.$$

~~~
- Generalize the sigmoid function to a $\softmax$ function, such that
  $$\softmax(→z)_i = \frac{e^{z_i}}{∑_j e^{z_j}}.$$

~~~
  Note that the original sigmoid function can be written as
  $$σ(x) = \softmax\big([x~~0]\big)_0 = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}.$$

~~~
The resulting classifier is also known as _multinomial logistic regression_,
_maximum entropy classifier_ or _softmax regression_.

---
# Multiclass Logistic Regression

Note that as defined, the multiclass logistic regression is overparametrized.
It is possible to generate only $K-1$ outputs and define $z_K = 0$, analously to
binary logistic regression.

~~~
In this settings, analogously to binary logistic regression, we can recover the
interpretation of $\softmax$ inputs as _logits_:

$$\softmax(→z)_i = \log\left(\frac{P(C_i | →x; →w)}{P(C_K | →x; →w)}\right).$$

---
# Multiclass Logistic Regression

Using the $\softmax$ function, we naturally define that
$$P(C_i | →x; ⇉W) = \softmax(⇉W_i →x)i = \frac{e^{⇉W_i →x}}{∑_j e^{⇉W_j →x}}.$$

~~~
We can then use MLE and train the model using stochastic gradient descent.

<div class="algorithm">

**Input**: Input dataset ($⇉X ∈ ℝ^{N×D}$, $→t ∈ \{0, 1, …, K-1\}$), learning rate $α ∈ ℝ^+$.<br>

- $→w ← 0$
- until convergence (or until patience is over), process batch of $N$ examples:
  - $g ← - \tfrac{1}{N} ∑_i ∇_→w \log(P(C_{t_i} | →x; →w)$
  - $→w ← →w - α→g$
</div>

---
# Multiclass Logistic Regression

Note that the decision regions of the multiclass logistic regression are singly
connected and convex.

![w=50%,h=center](../03/classification_convex_regions.pdf)

---
section: MSEasMLE
# Mean Square Error as MLE

During regression, we predict a number, not a real probability distribution.
In order to generate a distribution, we might consider a distribution with
the mean of the predicted value and a fixed variance $σ^2$ – the most general
such a distribution is the normal distribution.

---
section: MSEasMLE
# Mean Square Error as MLE

Therefore, assume our model generates a distribution
$$P(y | →x; →w) = 𝓝(y; f(→x; →w), σ^2).$$

~~~
Now we can apply MLE and get
$$\begin{aligned}
\argmax_→w P(𝕏; →w) =& \argmin_→w ∑_{i=1}^m -\log P(y_i | →x_i ; →w) \\
                         =& -\argmin_→w ∑_{i=1}^m \log \sqrt{\frac{1}{2πσ^2}}
                            e ^ {\normalsize -\frac{(y_i - f(→x_i; →w))^2}{2σ^2}} \\
                         =& -\argmin_→w {\color{gray} m \log (2πσ^2)^{-1/2} +}
                            ∑_{i=1}^m -\frac{(y_i - f(→x_i; →w))^2}{2σ^2} \\
                         =& \argmin_→w ∑_{i=1}^m \frac{(y_i - f(→x_i; →w))^2}{2σ^2} = \argmin_→w ∑_{i=1}^m (y_i - f(→x_i; →w))^2.
\end{aligned}$$

---
# Short-term Future Plans

- Derive that normal distribution is really the one with maximum entropy given
  mean and variance.

- Derive that softmax is the one with maximum entropy given a reasonable
  constraint.

- Support Vector Machines (allow efficient handling of user-prescribed
  non-linear transformation of inputs).

- Decision trees, random forests, gradient boosted decision trees.

- K-means, mixture of Gaussians, EM
